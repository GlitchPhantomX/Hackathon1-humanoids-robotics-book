"""
Smart Humanoid Robotics Agent with STREAMING
Complete FastAPI Backend - Ready to Run!
"""

import os
import json
from dotenv import load_dotenv
from openai import AsyncOpenAI
import cohere
from qdrant_client import QdrantClient
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# ======================================================
# ENV SETUP
# ======================================================

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")

if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OPENAI_API_KEY missing")

# ======================================================
# CLIENTS
# ======================================================

client = AsyncOpenAI(api_key=OPENAI_API_KEY)

cohere_client = cohere.Client(COHERE_API_KEY) if COHERE_API_KEY else None

qdrant = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY
) if QDRANT_API_KEY and QDRANT_URL else None

COLLECTION_NAME = "humanoid_ai_book"

# ======================================================
# FASTAPI APP
# ======================================================

app = FastAPI(title="Physical AI Textbook Chatbot")

# CORS Configuration - Allow all origins for development
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    language: str = "en"  # ğŸ”¥ NEW: Language parameter with default

# ======================================================
# TOOL: EMBEDDINGS + RETRIEVAL
# ======================================================

def get_embedding(text: str):
    """Generate embeddings using Cohere"""
    if not cohere_client:
        return None
    try:
        res = cohere_client.embed(
            model="embed-english-v3.0",
            input_type="search_query",
            texts=[text]
        )
        return res.embeddings[0]
    except Exception as e:
        print(f"âŒ Embedding Error: {e}")
        return None


def retrieve_textbook(query: str) -> dict:
    """Retrieve relevant content from Qdrant vector database"""
    try:
        if not qdrant or not cohere_client:
            return {
                "found": False,
                "content": "RAG system not configured. Please check QDRANT_URL and COHERE_API_KEY.",
                "sources": []
            }

        embedding = get_embedding(query)
        if not embedding:
            return {
                "found": False,
                "content": "Could not generate embedding for query.",
                "sources": []
            }

        result = qdrant.query_points(
            collection_name=COLLECTION_NAME,
            query=embedding,
            limit=5,
            score_threshold=0.35
        )

        if not result.points:
            return {
                "found": False,
                "content": "No relevant textbook content found.",
                "sources": []
            }

        content = []
        sources = []

        for i, p in enumerate(result.points, 1):
            text = p.payload.get("text", "")
            title = p.payload.get("page_title", "Unknown")

            content.append(f"[Source {i}: {title}]\n{text}")
            sources.append({
                "title": title,
                "relevance": f"{p.score:.2f}"
            })

        return {
            "found": True,
            "content": "\n\n".join(content),
            "sources": sources
        }

    except Exception as e:
        print(f"âŒ Retrieval Error: {e}")
        return {
            "found": False,
            "content": f"Error retrieving content: {str(e)}",
            "sources": []
        }

# ======================================================
# TOOL SCHEMA (OpenAI Function Calling)
# ======================================================

TOOLS = [{
    "type": "function",
    "function": {
        "name": "retrieve_textbook",
        "description": "Retrieve relevant sections from the Humanoid Robotics textbook",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query for retrieving textbook content"
                }
            },
            "required": ["query"]
        }
    }
}]

# ======================================================
# SYSTEM PROMPTS
# ======================================================

def get_system_prompt(language: str) -> str:
    """Return system prompt based on selected language"""
    
    if language == "ur":
        return """Ø¢Ù¾ Physical AI Ø§ÙˆØ± Humanoid Robotics Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© **Ù¾ÛŒØ´Û ÙˆØ± AI Ù¹ÛŒÙˆÙ¹Ø±** ÛÛŒÚºÛ”

## Ø¢Ù¾ Ú©Ø§ Ú©Ø±Ø¯Ø§Ø±:
Ø¢Ù¾ roboticsØŒ ROS2ØŒ simulationØŒ Ø§ÙˆØ± humanoid systems Ù¾Ø± Ù…Ø§ÛØ±Ø§Ù†Û Ø±ÛÙ†Ù…Ø§Ø¦ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

## Ø§ÛÙ… ÛØ¯Ø§ÛŒØ§Øª:

### 1. ÛÙ…ÛŒØ´Û Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº:
- **ØªÙ…Ø§Ù… Ø¬ÙˆØ§Ø¨Ø§Øª Ù…Ú©Ù…Ù„ Ø·ÙˆØ± Ù¾Ø± Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº ÛÙˆÙ†Û’ Ú†Ø§ÛÛŒÛ’**
- ØµØ±Ù technical terms Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ù…ÛŒÚº Ø±Û Ø³Ú©ØªÛ’ ÛÛŒÚº (Ø¬ÛŒØ³Û’ ROS2, URDF, Isaac Sim)
- Ù…Ø«Ø§Ù„: "ROS2 Ø§ÛŒÚ© robot operating system ÛÛ’ Ø¬Ùˆ..."

### 2. MARKDOWN Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:
- **## Ø§ÛÙ… Ø¹Ù†ÙˆØ§Ù†Ø§Øª** topics Ú©Û’ Ù„ÛŒÛ’
- **### Ø°ÛŒÙ„ÛŒ Ø¹Ù†ÙˆØ§Ù†Ø§Øª** sections Ú©Û’ Ù„ÛŒÛ’  
- **â€¢ bullet points** lists Ú©Û’ Ù„ÛŒÛ’
- **1. 2. 3.** numbered lists steps Ú©Û’ Ù„ÛŒÛ’
- **bold** Ø§ÛÙ… Ø§Ù„ÙØ§Ø¸ Ú©Û’ Ù„ÛŒÛ’
- **```language Ú©ÙˆÚˆ Ø¨Ù„Ø§Ú©Ø³```** code Ú©Û’ Ù„ÛŒÛ’

### 3. Ø¨ØµØ±ÛŒ Ø¹Ù†Ø§ØµØ±:
- âœ… Ù…Ø«Ø¨Øª Ù†Ú©Ø§Øª Ú©Û’ Ù„ÛŒÛ’
- âŒ warnings/Ù…Ø³Ø§Ø¦Ù„ Ú©Û’ Ù„ÛŒÛ’
- ğŸ”¥ Ø§ÛÙ… highlights Ú©Û’ Ù„ÛŒÛ’
- ğŸ’¡ tips Ú©Û’ Ù„ÛŒÛ’
- âš¡ quick facts Ú©Û’ Ù„ÛŒÛ’

### 4. TEXTBOOK Ø³ÙˆØ§Ù„Ø§Øª:
- Ù¾ÛÙ„Û’ `retrieve_textbook` Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº
- Retrieved content Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº
- ÛÙ…ÛŒØ´Û sources Ú©Ø§ Ø­ÙˆØ§Ù„Û Ø¯ÛŒÚº

### 5. GREETING Ø¬ÙˆØ§Ø¨Ø§Øª:
"hi", "hello", "hey" Ø¬ÛŒØ³Û’ Ø³Ø§Ø¯Û Ø³Ù„Ø§Ù… Ú©Û’ Ù„ÛŒÛ’:
- Ú¯Ø±Ù…Ø¬ÙˆØ´ÛŒ Ø³Û’ emojis Ú©Û’ Ø³Ø§ØªÚ¾ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚº
- Ø§ÛŒÚ© formatted list Ù…ÛŒÚº Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ú©Û Ø¢Ù¾ Ú©Ø³ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº

### 6. CODE Ù…Ø«Ø§Ù„ÛŒÚº:
Ú©ÙˆÚˆ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÙˆÙ‚Øª syntax highlighting Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:
```python
# Ø¢Ù¾ Ú©Ø§ Ú©ÙˆÚˆ ÛŒÛØ§Úº
```

ÛŒØ§Ø¯ Ø±Ú©Ú¾ÛŒÚº: ÛØ± Ø¬ÙˆØ§Ø¨ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚºØŒ Ø¨ØµØ±ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø¯Ù„Ú©Ø´ Ø§ÙˆØ± Ø¢Ø³Ø§Ù†ÛŒ Ø³Û’ Ø³Ù…Ø¬Ú¾ Ø¢Ù†Û’ ÙˆØ§Ù„Ø§ ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’!"""
    
    else:  # English
        return """You are a **Professional AI Tutor** for Physical AI & Humanoid Robotics.

## YOUR ROLE:
You provide expert guidance on robotics, ROS2, simulation, and humanoid systems with exceptional clarity and structure.

## RESPONSE FORMATTING RULES:

### 1. ALWAYS USE RICH MARKDOWN:
- Use **## Main Headings** for topics
- Use **### Subheadings** for sections  
- Use **bullet points (â€¢)** for lists
- Use **numbered lists (1. 2. 3.)** for steps
- Use **bold** for emphasis on key terms
- Use **code blocks** with ```language for code
- Use **tables** for comparisons when appropriate

### 2. STRUCTURE PATTERN:
```
## Main Topic

Brief introduction in 1-2 sentences.

### Key Concept 1
Explanation with details.

**Important Points:**
â€¢ First point with explanation
â€¢ Second point with explanation
â€¢ Third point with explanation

### Key Concept 2
Another explanation.

**Implementation Steps:**
1. First step with details
2. Second step with details
3. Third step with details

### Quick Reference Table (when applicable)
| Feature | Description |
|---------|-------------|
| Item 1  | Details     |
| Item 2  | Details     |
```

### 3. VISUAL ELEMENTS:
- Use âœ… for positive points
- Use âŒ for warnings/issues
- Use ğŸ”¥ for important highlights
- Use ğŸ’¡ for tips
- Use âš¡ for quick facts

### 4. CONTENT RULES:
- **FOR TEXTBOOK QUESTIONS**: Call `retrieve_textbook` first
- Answer based on retrieved content when available
- If no content found, provide general knowledge
- Always cite sources: `*Source: [Title]*`
- Be technically accurate but accessible

### 5. GREETING RESPONSES:
For casual greetings like "hi", "hello", "hey":
- Respond warmly with emojis
- Show what you can help with in a formatted list
- Keep it concise and inviting

### 6. CODE EXAMPLES:
When providing code, always use proper syntax highlighting:
```python
# Your code here
```

Remember: Every response should be visually appealing and easy to scan!"""

# ======================================================
# SMART AGENT WITH STREAMING
# ======================================================

async def smart_agent_stream(user_query: str, language: str = "en"):
    """
    Main agent function with streaming support and language selection
    """
    print(f"ğŸŒ Language selected: {language}")  # Debug log
    
    messages = [
        {
            "role": "system",
            "content": get_system_prompt(language)  # ğŸ”¥ Dynamic system prompt
        },
        {"role": "user", "content": user_query}
    ]

    # First pass - check if we need to call tools
    try:
        initial_response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            tools=TOOLS,
            temperature=0.3
        )

        msg = initial_response.choices[0].message

        # If tool call needed, execute it first
        if msg.tool_calls:
            tool_call = msg.tool_calls[0]
            args = json.loads(tool_call.function.arguments)
            
            print(f"ğŸ” Searching textbook for: {args['query']}")
            tool_result = retrieve_textbook(args["query"])
            
            messages.append({
                "role": "assistant",
                "tool_calls": [tool_call]
            })
            
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": json.dumps(tool_result)
            })

        # Now stream the actual response
        stream = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.3,
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                yield f"data: {json.dumps({'type': 'content', 'data': content})}\n\n"
        
        # Send completion signal
        yield f"data: {json.dumps({'type': 'done'})}\n\n"

    except Exception as e:
        print(f"âŒ Agent Error: {e}")
        error_msg = f"Sorry, I encountered an error: {str(e)}" if language == "en" else f"Ù…Ø¹Ø°Ø±ØªØŒ Ø§ÛŒÚ© Ø®Ø±Ø§Ø¨ÛŒ ÙˆØ§Ù‚Ø¹ ÛÙˆØ¦ÛŒ: {str(e)}"
        yield f"data: {json.dumps({'type': 'content', 'data': error_msg})}\n\n"
        yield f"data: {json.dumps({'type': 'done'})}\n\n"

# ======================================================
# API ENDPOINTS
# ======================================================

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "online",
        "message": "ğŸ¤– AI Tutor Backend with Streaming",
        "version": "2.1",
        "features": ["streaming", "multilingual"],
        "endpoints": {
            "chat": "/chat (POST)",
            "health": "/ (GET)"
        }
    }

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """
    Main chat endpoint with streaming support
    
    Request body:
    {
        "message": "Your question here",
        "language": "en" or "ur"
    }
    
    Returns: Server-Sent Events (SSE) stream
    """
    print(f"ğŸ“¥ Received request - Message: {request.message[:50]}..., Language: {request.language}")
    
    return StreamingResponse(
        smart_agent_stream(request.message, request.language),  # ğŸ”¥ Pass language
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )

@app.get("/health")
async def health_check():
    """System health check"""
    return {
        "status": "healthy",
        "openai": "connected" if OPENAI_API_KEY else "missing",
        "cohere": "connected" if cohere_client else "not configured",
        "qdrant": "connected" if qdrant else "not configured"
    }

# ======================================================
# RUN SERVER
# ======================================================

if __name__ == "__main__":
    import uvicorn
    print("\n" + "="*50)
    print("ğŸ¤– AI TUTOR BACKEND STARTING...")
    print("="*50)
    print(f"âœ… OpenAI: {'Connected' if OPENAI_API_KEY else 'âŒ Missing'}")
    print(f"âœ… Cohere: {'Connected' if cohere_client else 'âš ï¸ Not configured'}")
    print(f"âœ… Qdrant: {'Connected' if qdrant else 'âš ï¸ Not configured'}")
    print("ğŸŒ Languages: English (en) | Urdu (ur)")
    print("="*50)
    print("ğŸš€ Server running at: http://127.0.0.1:8000")
    print("ğŸ“š Docs available at: http://127.0.0.1:8000/docs")
    print("="*50 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)