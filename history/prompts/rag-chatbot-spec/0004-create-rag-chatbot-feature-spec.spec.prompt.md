--- id: 0004
title: Create RAG Chatbot Feature Spec
stage: spec
date: 2025-12-05
surface: agent
model: gemini
feature: rag-chatbot-spec
branch: 1-rag-chatbot-spec
user: user
command: /sp.specify
labels: [rag-chatbot, specification]
links:
  spec: specs/1-rag-chatbot-spec/spec.md
  ticket: null
  adr: null
  pr: null
files:
 - specs/1-rag-chatbot-spec/spec.md
 - specs/1-rag-chatbot-spec/checklists/requirements.md
tests:
  (none)
---

## Prompt

# RAG Chatbot Technical Specification ## Physical AI & Humanoid Robotics Textbook **Version:** 1.0 **Date:** December 5, 2025 **Project Root:** `C:\new\physical-ai-robotics-textbook\docusaurus` ---
## 1. PROJECT STRUCTURE ### 1.1 Directory Organization All RAG chatbot components MUST be integrated within the existing Docusaurus structure: ``` C:\new\physical-ai-robotics-textbook\docusaurus\ 
â”œâ”€â”€ src\ 
â”‚ â”œâ”€â”€ components\ 
â”‚ â”‚ â”œâ”€â”€ ChatBot\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatBot.tsx # Main chatbot component 
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatBot.module.css # Chatbot styles 
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatWidget.tsx # Minimizable widget wrapper 
â”‚ â”‚ â”‚ â”œâ”€â”€ MessageList.tsx # Chat message display 
â”‚ â”‚ â”‚ â”œâ”€â”€ MessageInput.tsx # User input component 
â”‚ â”‚ â”‚ â”œâ”€â”€ SelectionHandler.tsx # Text selection integration 
â”‚ â”‚ â”‚ â”œâ”€â”€ CitationLink.tsx # Source citation component 
â”‚ â”‚ â”‚ â””â”€â”€ types.ts # TypeScript interfaces 
â”‚ â”‚ â””â”€â”€ [other existing components...] 
â”‚ â”œâ”€â”€ pages\ 
â”‚ â”‚ â””â”€â”€ api\ 
â”‚ â”‚ â””â”€â”€ chat.ts # API route (if using Docusaurus API) 
â”‚ â””â”€â”€ theme\ 
â”‚ â””â”€â”€ Root.tsx # Global chatbot injection point 
â”œâ”€â”€ static\ 
â”‚ â””â”€â”€ chatbot\ 
â”‚ â”œâ”€â”€ icons\ 
â”‚ â”‚ â”œâ”€â”€ bot-icon.svg 
â”‚ â”‚ â”œâ”€â”€ send-icon.svg 
â”‚ â”‚ â””â”€â”€ minimize-icon.svg 
â”‚ â””â”€â”€ audio\ 
â”‚ â””â”€â”€ notification.mp3 
â”œâ”€â”€ backend\ 
â”‚ â”œâ”€â”€ main.py # FastAPI application entry 
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies 
â”‚ â”œâ”€â”€ .env.example # Environment variables template 
â”‚ â”œâ”€â”€ app\ 
â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”œâ”€â”€ config.py # Configuration management 
â”‚ â”‚ â”œâ”€â”€ models\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”‚ â”œâ”€â”€ database.py # Neon Postgres connection 
â”‚ â”‚ â”‚ â””â”€â”€ schemas.py # Pydantic models 
â”‚ â”‚ â”œâ”€â”€ services\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”‚ â”œâ”€â”€ qdrant_service.py # Vector DB operations 
â”‚ â”‚ â”‚ â”œâ”€â”€ openai_service.py # OpenAI API integration 
â”‚ â”‚ â”‚ â”œâ”€â”€ rag_pipeline.py # RAG orchestration 
â”‚ â”‚ â”‚ â””â”€â”€ analytics_service.py # Usage tracking 
â”‚ â”‚ â”œâ”€â”€ routes\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”‚ â”œâ”€â”€ chat.py # Chat endpoints 
â”‚ â”‚ â”‚ â””â”€â”€ health.py # Health check 
â”‚ â”‚ â””â”€â”€ utils\ 
â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”œâ”€â”€ embeddings.py # Embedding utilities 
â”‚ â”‚ â”œâ”€â”€ chunking.py # Text chunking logic 
â”‚ â”‚ â””â”€â”€ logger.py # Logging configuration 
â”‚ â””â”€â”€ scripts\ 
â”‚ â”œâ”€â”€ index_textbook.py # Populate Qdrant with content 
â”‚ â”œâ”€â”€ test_pipeline.py # RAG pipeline testing 
â”‚ â””â”€â”€ migrate_db.py # Database migrations 
â”œâ”€â”€ docs\ 
â”‚ â””â”€â”€ [existing textbook content...] 
â”œâ”€â”€ docusaurus.config.js # Add chatbot client module 
â”œâ”€â”€ package.json # Add frontend dependencies 
â””â”€â”€ README_CHATBOT.md # Chatbot setup guide ``` ### 1.2 Key Integration Points #### A. Frontend Integration (Docusaurus) **Location:** `src/theme/Root.tsx` ```tsx import React from 'react'; import ChatWidget from '@site/src/components/ChatBot/ChatWidget'; export default function Root({children}) { return ( <> {children} <ChatWidget /> </> ); } ``` #### B. Backend Deployment **Separate Repository:** Backend will be deployed independently but configured to accept requests from Docusaurus domain **CORS Configuration:** ```python # backend/main.py from fastapi.middleware.cors import CORSMiddleware app.add_middleware( CORSMiddleware, allow_origins=[ "https://your-textbook-domain.com", "http://localhost:3000" # Development ], allow_credentials=True, allow_methods=["*"], allow_headers=["*"], ) ``` --- ## 2. COMPONENT SPECIFICATIONS ### 2.1 Frontend Components #### A. ChatBot.tsx **Path:** `src/components/ChatBot/ChatBot.tsx` **Responsibilities:** - Main chat interface container - Message state management - API communication with backend - Error handling and loading states **Key Props:** ```typescript interface ChatBotProps { initialOpen?: boolean; selectedText?: string; selectionContext?: { chapter: string; section: string; }; } ``` **State Management:** ```typescript interface ChatState { messages: Message[]; isLoading: boolean; error: string | null; mode: 'general' | 'selected_text'; sessionId: string; } ``` #### B. SelectionHandler.tsx **Path:** `src/components/ChatBot/SelectionHandler.tsx` **Functionality:** - Listen for text selection events on documentation pages - Context menu integration (right-click) - Keyboard shortcut handler (Ctrl+Shift+A) - Extract surrounding context (chapter/section) **Implementation Pattern:** ```typescript useEffect(() => { const handleSelection = () => { const selection = window.getSelection(); if (selection && selection.toString().length > 10) { // Show "Ask about this" option setSelectedText(selection.toString()); setSelectionContext(extractContext()); } }; document.addEventListener('mouseup', handleSelection); return () => document.removeEventListener('mouseup', handleSelection); }, []); ``` #### C. CitationLink.tsx **Path:** `src/components/ChatBot/CitationLink.tsx` **Purpose:** Render clickable source citations **Props:** ```typescript interface CitationLinkProps { chapter: string; section: string; onClick: () => void; } ``` **Output Format:** ``` [Chapter 2, Section 2.3] â†’ Clicking navigates to /docs/chapter-2#section-2-3 ``` ### 2.2 Backend Services #### A. RAG Pipeline Service **Path:** `backend/app/services/rag_pipeline.py` **Core Functions:** ```python class RAGPipeline: def __init__(self): self.qdrant = QdrantService() self.openai = OpenAIService() self.db = DatabaseService() async def process_general_query( self, query: str, session_id: str, user_background: dict = None ) -> ChatResponse: """ 1. Expand query with technical synonyms 2. Retrieve top-k chunks from Qdrant (k=5) 3. Rerank by relevance 4. Generate response with citations 5. Log to Neon Postgres """ pass async def process_selected_text_query( self, query: str, selected_text: str, context: dict, session_id: str ) -> ChatResponse: """ 1. Create ephemeral embedding from selection 2. Restrict retrieval to selection's chapter/section 3. Generate focused response 4. Ensure no context bleeding """ pass ``` #### B. Qdrant Service **Path:** `backend/app/services/qdrant_service.py` **Configuration:** ```python from qdrant_client import QdrantClient from qdrant_client.models import Distance, VectorParams class QdrantService: def __init__(self): self.client = QdrantClient( url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY") ) self.collection_name = "physical_ai_textbook_v1" def create_collection(self): self.client.create_collection( collection_name=self.collection_name, vectors_config=VectorParams( size=3072, # text-embedding-3-large dimension distance=Distance.COSINE ) ) async def search( self, query_vector: List[float], limit: int = 5, filters: dict = None ) -> List[ScoredPoint]: """Hybrid search with metadata filtering""" pass ``` **Metadata Structure:** ```python { "chapter": "Chapter 2", "section": "2.3 Isaac Sim Setup", "subsection": "Hardware Requirements", "type": "hardware_spec", # concept | code | hardware_spec | example "keywords": ["RTX", "GPU", "Isaac Sim", "NVIDIA"], "page_number": 45, "textbook_version": "1.0" } ``` #### C. OpenAI Service **Path:** `backend/app/services/openai_service.py` **Functions:** ```python from openai import AsyncOpenAI class OpenAIService: def __init__(self): self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY")) async def create_embedding(self, text: str) -> List[float]: """Generate embedding using text-embedding-3-large""" response = await self.client.embeddings.create( model="text-embedding-3-large", input=text ) return response.data[0].embedding async def generate_response( self, query: str, context_chunks: List[str], mode: str = "general" ) -> str: """Generate response using GPT-4 with retrieved context""" system_prompt = self._build_system_prompt(mode) user_prompt = self._build_user_prompt(query, context_chunks) response = await self.client.chat.completions.create( model="gpt-4-turbo-preview", messages=[ {"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt} ], temperature=0.3, max_tokens=800 ) return response.choices[0].message.content ``` **System Prompt Template:** ```python SYSTEM_PROMPT = """You are an expert teaching assistant for the Physical AI & Humanoid Robotics course. RULES: 1. Answer ONLY using the provided textbook context 2. ALWAYS cite sources in format: [Chapter X, Section Y.Z] 3. If context insufficient, explicitly state: "This detail is not covered in the provided textbook section." 4. Use technical precision (correct ROS 2 terminology, version numbers) 5. Explain complex concepts step-by-step 6. Include relevant code snippets when available in context CONTEXT FROM TEXTBOOK: {context_chunks} STUDENT QUESTION: {query} Provide a clear, educational response with proper citations.""" ``` --- ## 3. DATABASE SCHEMA (NEON POSTGRES) ### 3.1 Tables Definition **File:** `backend/app/models/database.py` ```sql -- User Sessions CREATE TABLE user_sessions ( session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(), user_id UUID NULL, -- NULL for anonymous users created_at TIMESTAMP DEFAULT NOW(), last_activity TIMESTAMP DEFAULT NOW(), user_background JSONB, -- {hardware_experience, programming_level} device_info JSONB -- User agent, screen size for analytics ); -- Query Logs CREATE TABLE query_logs ( query_id UUID PRIMARY KEY DEFAULT gen_random_uuid(), session_id UUID REFERENCES user_sessions(session_id) ON DELETE CASCADE, query_text TEXT NOT NULL, query_type VARCHAR(50) CHECK (query_type IN ('general', 'selected_text')), selected_context TEXT, -- Stored if selected_text mode selected_metadata JSONB, -- {chapter, section} retrieved_chunks JSONB, -- Array of chunk IDs used response_text TEXT, citations JSONB, -- [{chapter, section, url}] response_time_ms INTEGER, user_feedback INTEGER CHECK (user_feedback IN (-1, 0, 1)), created_at TIMESTAMP DEFAULT NOW(), INDEX idx_session (session_id), INDEX idx_created (created_at), INDEX idx_feedback (user_feedback) ); -- Popular Topics (Materialized View) CREATE MATERIALIZED VIEW popular_topics AS SELECT selected_metadata->>'chapter' AS chapter, selected_metadata->>'section' AS section, COUNT(*) AS query_count, AVG(CASE WHEN user_feedback = 1 THEN 1.0 ELSE 0.0 END) AS satisfaction_rate, MAX(created_at) AS last_queried FROM query_logs WHERE created_at > NOW() - INTERVAL '30 days' GROUP BY chapter, section ORDER BY query_count DESC; -- Refresh schedule: Every 6 hours ``` ### 3.2 SQLAlchemy Models **File:** `backend/app/models/schemas.py` ```python from sqlalchemy import Column, String, Integer, Float, DateTime, Text, ForeignKey from sqlalchemy.dialects.postgresql import UUID, JSONB from sqlalchemy.ext.declarative import declarative_base import uuid Base = declarative_base() class UserSession(Base): __tablename__ = "user_sessions" session_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4) user_id = Column(UUID(as_uuid=True), nullable=True) created_at = Column(DateTime, server_default=func.now()) last_activity = Column(DateTime, server_default=func.now(), onupdate=func.now()) user_background = Column(JSONB) device_info = Column(JSONB) class QueryLog(Base): __tablename__ = "query_logs" query_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4) session_id = Column(UUID(as_uuid=True), ForeignKey("user_sessions.session_id")) query_text = Column(Text, nullable=False) query_type = Column(String(50), nullable=False) selected_context = Column(Text) selected_metadata = Column(JSONB) retrieved_chunks = Column(JSONB) response_text = Column(Text) citations = Column(JSONB) response_time_ms = Column(Integer) user_feedback = Column(Integer) created_at = Column(DateTime, server_default=func.now()) ``` --- ## 4. API ENDPOINTS ### 4.1 Chat Endpoint **File:** `backend/app/routes/chat.py` ```python from fastapi import APIRouter, HTTPException, Depends from pydantic import BaseModel router = APIRouter(prefix="/api/v1", tags=["chat"]) class GeneralQueryRequest(BaseModel): query: str session_id: str user_background: dict = None class SelectedTextQueryRequest(BaseModel): query: str selected_text: str selection_context: dict # {chapter, section} session_id: str class ChatResponse(BaseModel): answer: str citations: List[dict] # [{chapter, section, url}] confidence: float mode: str response_time_ms: int @router.post("/chat/general", response_model=ChatResponse) async def general_query(request: GeneralQueryRequest): """Handle general textbook queries""" try: pipeline = RAGPipeline() response = await pipeline.process_general_query( query=request.query, session_id=request.session_id, user_background=request.user_background ) return response except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @router.post("/chat/selected", response_model=ChatResponse) async def selected_text_query(request: SelectedTextQueryRequest): """Handle queries about selected text""" if len(request.selected_text) > 2000: raise HTTPException( status_code=400, detail="Selected text exceeds 2000 character limit" ) pipeline = RAGPipeline() response = await pipeline.process_selected_text_query( query=request.query, selected_text=request.selected_text, context=request.selection_context, session_id=request.session_id ) return response @router.post("/chat/feedback") async def submit_feedback(query_id: str, feedback: int): """Record user feedback (thumbs up/down)""" # Update query_logs table pass ### 4.2 Health Check **File:** `backend/app/routes/health.py` ```python @router.get("/health") async def health_check(): """Check service health""" checks = { "qdrant": await check_qdrant_connection(), "postgres": await check_postgres_connection(), "openai": await check_openai_api() } if all(checks.values()): return {"status": "healthy", "checks": checks} else: raise HTTPException(status_code=503, detail=checks) ``` --- ## 5. INDEXING PIPELINE ### 5.1 Textbook Content Extraction **File:** `backend/scripts/index_textbook.py` **Process:** ```python import os from pathlib import Path from app.services.qdrant_service import QdrantService from app.services.openai_service import OpenAIService from app.utils.chunking import chunk_markdown async def index_textbook_content(): """ Extract and index all Markdown files from Docusaurus docs folder """ docs_path = Path("C:/new/physical-ai-robotics-textbook/docusaurus/docs") qdrant = QdrantService() openai = OpenAIService() # Ensure collection exists qdrant.create_collection() for md_file in docs_path.rglob("*.md"): print(f"Processing: {md_file}") # Extract metadata from frontmatter metadata = extract_frontmatter(md_file) # Read content with open(md_file, 'r', encoding='utf-8') as f: content = f.read() # Chunk content (800-1200 tokens, 200 overlap) chunks = chunk_markdown(content, metadata) # Create embeddings and upload to Qdrant for chunk in chunks: embedding = await openai.create_embedding(chunk.text) await qdrant.upsert_point( vector=embedding, payload={ "text": chunk.text, "chapter": chunk.chapter, "section": chunk.section, "subsection": chunk.subsection, "type": chunk.type, "keywords": chunk.keywords, "file_path": str(md_file.relative_to(docs_path)), "url": generate_doc_url(md_file) } ) print(f"Indexed {len(chunks)} chunks from {md_file.name}") def extract_frontmatter(md_file: Path) -> dict: """Parse YAML frontmatter from Markdown file""" import yaml with open(md_file, 'r', encoding='utf-8') as f: content = f.read() if content.startswith('---\n'): _, frontmatter, _ = content.split('---\n', 2) return yaml.safe_load(frontmatter) return {} def generate_doc_url(md_file: Path) -> str: """Convert file path to Docusaurus URL""" # Example: docs/chapter-2/section-2-3.md â†’ /docs/chapter-2/section-2-3 relative_path = md_file.relative_to(docs_path) url_path = str(relative_path).replace('\\', '/').replace('.md', '') return f"/docs/{url_path}" ``` ### 5.2 Chunking Strategy **File:** `backend/app/utils/chunking.py` ```python from typing import List import tiktoken class TextChunk: def __init__(self, text: str, metadata: dict): self.text = text self.chapter = metadata.get('chapter', 'Unknown') self.section = metadata.get('section', 'Unknown') self.subsection = metadata.get('subsection', '') self.type = self._classify_type(text) self.keywords = self._extract_keywords(text) def _classify_type(self, text: str) -> str: """Classify chunk type based on content patterns""" if '```' in text: return 'code' elif any(hw in text.lower() for hw in ['jetson', 'gpu', 'ram', 'ssd']): return 'hardware_spec' elif 'example:' in text.lower() or 'for instance' in text.lower(): return 'example' else: return 'concept' def _extract_keywords(self, text: str) -> List[str]: """Extract technical keywords for filtering""" # Simple keyword extraction (can be enhanced with NLP) keywords = [] technical_terms = ['ROS 2', 'Isaac Sim', 'Gazebo', 'VSLAM', 'Jetson', 'URDF'] for term in technical_terms: if term.lower() in text.lower(): keywords.append(term) return keywords def chunk_markdown(content: str, metadata: dict) -> List[TextChunk]: """ Chunk Markdown content with respect to structure Strategy: - Target: 800-1200 tokens per chunk - Overlap: 200 tokens - Respect boundaries: Headers, code blocks, lists """ encoding = tiktoken.get_encoding("cl100k_base") # Split by headers first sections = split_by_headers(content) chunks = [] for section in sections: section_metadata = {**metadata, 'section': section['title']} tokens = encoding.encode(section['content']) if len(tokens) <= 1200: # Section fits in one chunk chunks.append(TextChunk(section['content'], section_metadata)) else: # Split section into overlapping chunks chunk_size = 1000 overlap = 200 for i in range(0, len(tokens), chunk_size - overlap): chunk_tokens = tokens[i:i + chunk_size] chunk_text = encoding.decode(chunk_tokens) chunks.append(TextChunk(chunk_text, section_metadata)) return chunks def split_by_headers(content: str) -> List[dict]: """Split Markdown by headers (##, ###)""" import re header_pattern = r'^(#{2,3})\s+(.+)' lines = content.split('\n') sections = [] current_section = {'title': 'Introduction', 'content': ''} for line in lines: match = re.match(header_pattern, line, re.MULTILINE) if match: # Save previous section if current_section['content'].strip(): sections.append(current_section) # Start new section current_section = { 'title': match.group(2), 'content': '' } else: current_section['content'] += line + '\n' # Add last section if current_section['content'].strip(): sections.append(current_section) return sections ``` --- ## 6. DEPLOYMENT CONFIGURATION ### 6.1 Environment Variables **File:** `backend/.env.example` ```bash # OpenAI OPENAI_API_KEY=sk-proj-... # Qdrant Cloud QDRANT_URL=https://your-cluster.qdrant.io QDRANT_API_KEY=your-api-key QDRANT_COLLECTION=physical_ai_textbook_v1 # Neon Postgres DATABASE_URL=postgresql://user:password@ep-xxx.us-east-2.aws.neon.tech/chatbot_db?sslmode=require # CORS ALLOWED_ORIGINS=https://your-textbook-domain.com,http://localhost:3000 # Rate Limiting RATE_LIMIT_REQUESTS=50 # per minute per session RATE_LIMIT_WINDOW=60 # seconds # Monitoring SENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx # Optional LOG_LEVEL=INFO ``` ### 6.2 Backend Deployment (Railway) **File:** `backend/railway.json` ```json { "$schema": "https://railway.app/railway.schema.json", "build": { "builder": "NIXPACKS", "buildCommand": "pip install -r requirements.txt" }, "deploy": { "startCommand": "uvicorn main:app --host 0.0.0.0 --port $PORT", "healthcheckPath": "/health", "healthcheckTimeout": 30, "restartPolicyType": "ON_FAILURE", "restartPolicyMaxRetries": 3 } } ``` **File:** `backend/requirements.txt` ```txt fastapi==0.109.0 uvicorn[standard]==0.27.0 python-multipart==0.0.6 pydantic==2.5.3 pydantic-settings==2.1.0 # OpenAI openai==1.10.0 # Vector DB qdrant-client==1.7.3 # Database sqlalchemy==2.0.25 asyncpg==0.29.0 psycopg2-binary==2.9.9 alembic==1.13.1 # Utilities python-dotenv==1.0.0 tiktoken==0.5.2 pyyaml==6.0.1 httpx==0.26.0 # Monitoring sentry-sdk[fastapi]==1.40.0 ``` ### 6.3 Frontend Configuration **File:** `docusaurus/package.json` (additions) ```json { "dependencies": { "@docusaurus/core": "3.0.0", "@docusaurus/preset-classic": "3.0.0", // ... existing dependencies // Chatbot dependencies "axios": "^1.6.5", "uuid": "^9.0.1", "react-markdown": "^9.0.1", "remark-gfm": "^4.0.0", "highlight.js": "^11.9.0" } } ``` **File:** `docusaurus/docusaurus.config.js` (additions) ```javascript module.exports = { // ... existing config customFields: { chatbotApiUrl: process.env.CHATBOT_API_URL || 'http://localhost:8000', }, scripts: [ // Optional: Analytics for chatbot usage { src: 'https://plausible.io/js/script.js', defer: true, 'data-domain': 'your-textbook-domain.com' } ], }; ``` --- ## 7. TESTING REQUIREMENTS ### 7.1 Unit Tests **File:** `backend/tests/test_rag_pipeline.py` ```python import pytest from app.services.rag_pipeline import RAGPipeline @pytest.mark.asyncio async def test_general_query_with_citations(): """Test that general queries return proper citations""" pipeline = RAGPipeline() response = await pipeline.process_general_query( query="What is ROS 2?", session_id="test-session-123" ) assert response.answer is not None assert len(response.citations) > 0 assert all('chapter' in c for c in response.citations) assert response.confidence > 0.5 @pytest.mark.asyncio async def test_selected_text_no_context_bleeding(): """Ensure selected text mode doesn't retrieve from other chapters""" pipeline = RAGPipeline() selected_text = "NVIDIA Jetson Orin Nano specifications..." response = await pipeline.process_selected_text_query( query="Why is 64GB RAM recommended?", selected_text=selected_text, context={"chapter": "Chapter 4", "section": "Hardware"}, session_id="test-session-456" ) # All citations should be from Chapter 4 only assert all(c['chapter'] == "Chapter 4" for c in response.citations) ``` **File:** `backend/tests/test_chunking.py` ```python from app.utils.chunking import chunk_markdown def test_chunking_respects_token_limit(): """Test that chunks stay within 800-1200 token range""" content = "# Chapter 1\n\n" + "Lorem ipsum " * 2000 metadata = {"chapter": "Chapter 1"} chunks = chunk_markdown(content, metadata) for chunk in chunks: token_count = len(encoding.encode(chunk.text)) assert 500 <= token_count <= 1300 # Allow some buffer def test_chunking_preserves_code_blocks(): """Ensure code blocks aren't split mid-block""" content = """ ## Setup ```python def example(): return \"test\" ``` More text here. """ chunks = chunk_markdown(content, {"chapter": "Chapter 1"}) # Code block should be in one chunk code_chunks = [c for c in chunks if '```python' in c.text] assert any('def example():' in c.text and 'return \"test\"' in c.text for c in code_chunks) ``` ### 7.2 Integration Tests **File:** `backend/tests/test_api_endpoints.py` ```python from fastapi.testclient import TestClient from main import app client = TestClient(app) def test_general_query_endpoint(): """Test /api/v1/chat/general endpoint""" response = client.post( "/api/v1/chat/general", json={ "query": "What is Gazebo?", "session_id": "test-123" } ) assert response.status_code == 200 data = response.json() assert "answer" in data assert "citations" in data assert len(data["citations"]) > 0 def test_selected_text_query_endpoint(): """Test /api/v1/chat/selected endpoint""" response = client.post( "/api/v1/chat/selected", json={ "query": "Why this hardware?", "selected_text": "Jetson Orin Nano (8GB)...", "selection_context": { "chapter": "Chapter 4", "section": "Hardware Requirements" }, "session_id": "test-456" } ) assert response.status_code == 200 data = response.json() assert data["mode"] == "selected_text" def test_rate_limiting(): """Test that rate limiting works""" session_id = "rate-limit-test" # Send 60 requests (over limit of 50/min) responses = [] for i in range(60): response = client.post( "/api/v1/chat/general", json={"query": f"Test {i}", "session_id": session_id} ) responses.append(response.status_code) # Last 10 should be rate limited (429) assert 429 in responses[-10:] ``` ### 7.3 User Acceptance Testing **Test Cases Document:** `docs/CHATBOT_TEST_CASES.md` | Test ID | Scenario | Expected Result | Pass/Fail | |---------|----------|-----------------|-----------| | UAT-01 | Ask "What is ROS 2?" | Returns definition with [Chapter X] citation | | | UAT-02 | Select hardware specs paragraph, ask "Why Jetson?" | Answers from selection only | | | UAT-03 | Ask about topic not in textbook | Responds "This is not covered..." | | | UAT-04 | Send empty query | Shows validation error | | | UAT-05 | Select > 2000 chars, ask question | Returns 400 error with message | | | UAT-06 | Click citation link | Navigates to correct doc section | | | UAT-07 | Submit thumbs up/down feedback | Updates database record | | | UAT-08 | Open chatbot on mobile | Responsive full-screen layout | | | UAT-09 | Use keyboard shortcut (Ctrl+Shift+A) | Opens chatbot with selection | | | UAT-10 | Ask code-related question | Includes syntax-highlighted code snippet | | --- ## 8. MONITORING & ANALYTICS ### 8.1 Logging Configuration **File:** `backend/app/utils/logger.py` ```python import logging import sys from pythonjsonlogger import jsonlogger def setup_logger(): """Configure structured JSON logging""" logger = logging.getLogger("chatbot") logger.setLevel(logging.INFO) handler = logging.StreamHandler(sys.stdout) formatter = jsonlogger.JsonFormatter( fmt='%(asctime)s %(name)s %(levelname)s %(message)s', rename_fields={ "asctime": "timestamp", "levelname": "level" } ) handler.setFormatter(formatter) logger.addHandler(handler) return logger # Usage in services logger = setup_logger() logger.info("Query processed", extra={ "query_id": query_id, "response_time_ms": elapsed_time, "citations_count": len(citations) }) ``` ### 8.2 Analytics Dashboard Queries **File:** `backend/scripts/analytics_queries.sql` ```sql -- Most popular topics (last 30 days) SELECT selected_metadata->>'chapter' AS chapter, selected_metadata->>'section' AS section, COUNT(*) AS query_count, ROUND(AVG(response_time_ms)) AS avg_response_ms, ROUND(AVG(CASE WHEN user_feedback = 1 THEN 1.0 ELSE 0.0 END) * 100, 2) AS satisfaction_pct FROM query_logs WHERE created_at > NOW() - INTERVAL '30 days' GROUP BY chapter, section ORDER BY query_count DESC LIMIT 20; -- Daily active sessions SELECT DATE(created_at) AS date, COUNT(DISTINCT session_id) AS active_sessions, COUNT(*) AS total_queries, ROUND(AVG(response_time_ms)) AS avg_response_ms FROM query_logs WHERE created_at > NOW() - INTERVAL '90 days' GROUP BY DATE(created_at) ORDER BY date DESC; -- Query type distribution SELECT query_type, COUNT(*) AS count, ROUND(AVG(response_time_ms)) AS avg_response_ms, ROUND(AVG(CASE WHEN user_feedback = 1 THEN 1.0 ELSE 0.0 END) * 100, 2) AS satisfaction_pct FROM query_logs WHERE created_at > NOW() - INTERVAL '7 days' GROUP BY query_type; -- User feedback summary SELECT CASE WHEN user_feedback = 1 THEN 'Thumbs Up' WHEN user_feedback = -1 THEN 'Thumbs Down' ELSE 'No Feedback' END AS feedback_type, COUNT(*) AS count, ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage FROM query_logs WHERE created_at > NOW() - INTERVAL '30 days' GROUP BY user_feedback; ``` --- ## 9. MAINTENANCE PROCEDURES ### 9.1 Re-indexing Workflow **When to Re-index:** - Textbook content updated (new chapters, corrections) - Embedding model upgraded (e.g., text-embedding-3-large â†’ text-embedding-4) - Chunking strategy changed **Process:** ```bash # 1. Backup existing Qdrant collection python backend/scripts/backup_qdrant.py # 2. Create new collection with version suffix # Modify QDRANT_COLLECTION in .env: physical_ai_textbook_v2 # 3. Run indexing script python backend/scripts/index_textbook.py # 4. Validate new collection python backend/scripts/test_pipeline.py --collection physical_ai_textbook_v2 # 5. Update production environment variable # Switch QDRANT_COLLECTION to v2 # 6. Monitor for 24 hours, delete old collection if stable ``` ### 9.2 Cost Optimization **Monthly Cost Estimate (Free Tier):** - **Qdrant Cloud**: Free (1GB storage, 1M vectors) - **Neon Postgres**: Free (0.5GB storage, 100 hours compute) - **OpenAI API**: ~$20-50 (assuming 10k queries/month) - Embeddings: ~$0.50 (text-embedding-3-large) - Completions: ~$20-50 (GPT-4-turbo at $0.01/1K tokens) **Optimization Strategies:** 1. **Caching**: Store embeddings of common queries 2. **Response Caching**: Cache responses for identical queries (30 min TTL) 3. **Batch Embeddings**: When re-indexing, batch requests (50 chunks per API call) 4. **Model Selection**: Use GPT-3.5-turbo for simple queries (detected by keyword matching) ```python # Example caching implementation from functools import lru_cache import hashlib @lru_cache(maxsize=1000) def get_cached_response(query_hash: str): """Cache responses for 30 minutes""" # Check Redis/in-memory cache pass async def process_query(query: str): query_hash = hashlib.md5(query.encode()).hexdigest() cached = get_cached_response(query_hash) if cached: return cached # Generate new response response = await pipeline.process_general_query(query) cache_response(query_hash, response, ttl=1800) # 30 min return response ``` --- ## 10. SUCCESS METRICS ### 10.1 Performance KPIs | Metric | Target | Measurement | |--------|--------|-------------| | **Response Time (P95)** | < 3 seconds | Monitor `response_time_ms` column | | **Accuracy** | > 90% | Manual review of 100 random samples/week | | **Citation Rate** | 100% | Automated check: all responses must include `[Chapter X]` | | **User Satisfaction** | > 70% thumbs up | `AVG(user_feedback)` from query_logs | | **Error Rate** | < 2% | 5xx errors / total requests | | **Uptime** | > 99% | Railway/Vercel monitoring | ### 10.2 Usage Metrics | Metric | Target | Measurement | |--------|--------|-------------| | **Daily Active Users** | 50+ | `COUNT(DISTINCT session_id)` per day | | **Queries per Session** | 3+ | `AVG(queries/session)` | | **Engagement Rate** | > 30% of page visitors | Google Analytics + chatbot opens | | **Selected Text Usage** | > 20% of queries | `query_type='selected_text' / total` | ### 10.3 Quality Assurance **Weekly Review Process:** 1. Sample 50 random queries with low feedback scores 2. Evaluate response quality (accuracy, citation correctness) 3. Identify common failure patterns 4. Update prompt templates or re-index if needed 5. Document improvements in changelog **Quarterly Review:** - Analyze popular topics â†’ Suggest textbook enhancements - Review cost trends â†’ Optimize if exceeding $100/month - User surveys â†’ Collect qualitative feedback - A/B test prompt variations --- ## 11. DEPLOYMENT CHECKLIST ### 11.1 Pre-Launch (Complete Before Production) **Backend Setup:** - [ ] Qdrant collection created and populated with all textbook chapters - [ ] Neon Postgres database schema deployed - [ ] OpenAI API key configured in Railway environment - [ ] CORS configured for production domain - [ ] Rate limiting middleware enabled (50 req/min per session) - [ ] Health check endpoint returning 200 - [ ] Sentry error tracking configured - [ ] SSL certificate valid **Frontend Setup:** - [ ] ChatBot component integrated in `src/theme/Root.tsx` - [ ] Text selection handler tested on all documentation pages - [ ] Citation links navigate correctly to docs - [ ] Mobile responsive design verified (< 768px width) - [ ] Loading states implemented (skeleton screens) - [ ] Error messages user-friendly - [ ] Keyboard shortcuts functional (Ctrl+Shift+A) **Testing:** - [ ] All unit tests passing (`pytest backend/tests/`) - [ ] Integration tests passing (API endpoints) - [ ] 20 user acceptance test cases completed - [ ] Load testing: 100 concurrent requests handled - [ ] Browser compatibility: Chrome, Firefox, Safari, Edge **Documentation:** - [ ] README_CHATBOT.md created with setup instructions - [ ] API documentation generated (FastAPI auto-docs) - [ ] Analytics dashboard queries documented - [ ] Runbook for common issues created ### 11.2 Launch Day **Deployment Steps:** 1. Deploy backend to Railway 2. Update `CHATBOT_API_URL` in Docusaurus environment 3. Deploy Docusaurus site to production 4. Monitor logs for first 100 queries 5. Test from production domain 6. Announce to users (in-app banner) **Monitoring:** - [ ] Check Railway logs every hour (first 24h) - [ ] Monitor Sentry for errors - [ ] Track response times in Neon Postgres - [ ] Review first 50 user feedback scores ### 11.3 Post-Launch (Week 1) - [ ] Collect user feedback survey responses - [ ] Analyze query logs for common patterns - [ ] Identify and fix any edge cases - [ ] Update FAQ based on common queries - [ ] Prepare iteration plan for improvements --- ## 12. APPENDIX ### 12.1 Example Frontend Component **File:** `src/components/ChatBot/ChatBot.tsx` ```typescript import React, { useState, useEffect, useRef } from 'react'; import axios from 'axios'; import { v4 as uuidv4 } from 'uuid'; import styles from './ChatBot.module.css'; import MessageList from './MessageList'; import MessageInput from './MessageInput'; interface Message { id: string; role: 'user' | 'assistant'; content: string; citations?: Citation[]; timestamp: Date; } interface Citation { chapter: string; section: string; url: string; } interface ChatBotProps { selectedText?: string; selectionContext?: { chapter: string; section: string; }; } const ChatBot: React.FC<ChatBotProps> = ({ selectedText, selectionContext }) => { const [messages, setMessages] = useState<Message[]>([]); const [isLoading, setIsLoading] = useState(false); const [sessionId] = useState(() => uuidv4()); const messagesEndRef = useRef<HTMLDivElement>(null); const apiUrl = process.env.CHATBOT_API_URL || 'http://localhost:8000'; useEffect(() => { // Show welcome message if (messages.length === 0) { setMessages([{ id: uuidv4(), role: 'assistant', content: selectedText ? `I'll help you understand this section. What would you like to know?` : `Hi! I'm your Physical AI textbook assistant. Ask me anything about ROS 2, Isaac Sim, or humanoid robotics!`, timestamp: new Date() }]); } }, [selectedText]); useEffect(() => { messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' }); }, [messages]); const handleSendMessage = async (content: string) => { // Add user message const userMessage: Message = { id: uuidv4(), role: 'user', content, timestamp: new Date() }; setMessages(prev => [...prev, userMessage]); setIsLoading(true); try { const endpoint = selectedText ? `${apiUrl}/api/v1/chat/selected` : `${apiUrl}/api/v1/chat/general`; const payload = selectedText ? { query: content, selected_text: selectedText, selection_context: selectionContext, session_id: sessionId } : { query: content, session_id: sessionId }; const response = await axios.post(endpoint, payload); // Add assistant message const assistantMessage: Message = { id: uuidv4(), role: 'assistant', content: response.data.answer, citations: response.data.citations, timestamp: new Date() }; setMessages(prev => [...prev, assistantMessage]); } catch (error) { console.error('Chat error:', error); // Show error message setMessages(prev => [...prev, { id: uuidv4(), role: 'assistant', content: 'Sorry, I encountered an error. Please try again or rephrase your question.', timestamp: new Date() }]); } finally { setIsLoading(false); } }; const handleFeedback = async (messageId: string, feedback: number) => { try { await axios.post(`${apiUrl}/api/v1/chat/feedback`, { query_id: messageId, feedback }); } catch (error) { console.error('Feedback error:', error); } }; return ( <div className={styles.chatContainer}> <div className={styles.header}> <h3>ðŸ“š Textbook Assistant</h3> {selectedText && ( <span className={styles.modeIndicator}> Selection Mode </span> )} </div> <MessageList messages={messages} onFeedback={handleFeedback} /> <div ref={messagesEndRef} /> <MessageInput onSend={handleSendMessage} disabled={isLoading} placeholder={selectedText ? "Ask about this section..." : "Ask a question about the textbook..." } /> </div> ); }; export default ChatBot; ``` ### 12.2 Quick Start Commands ```bash # Backend Setup cd C:\new\physical-ai-robotics-textbook\docusaurus\backend python -m venv venv venv\Scripts\activate # Windows pip install -r requirements.txt cp .env.example .env # Edit .env with your API keys # Index textbook content python scripts/index_textbook.py # Run backend locally uvicorn main:app --reload --port 8000 # Frontend Setup (in separate terminal) cd C:\new\physical-ai-robotics-textbook\docusaurus npm install npm start # Starts on http://localhost:3000 # Run tests cd backend pytest tests/ -v # Deploy backend to Railway railway login railway link railway up # Deploy frontend to Vercel/GitHub Pages npm run build # Follow deployment platform instructions ``` --- ## DOCUMENT CONTROL **Version:** 1.0 **Last Updated:** December 5, 2025 **Next Review:** January 5, 2026 **Owner:** Development Team **Approvers:** [Pending] --- *This specification provides a complete implementation guide for integrating the RAG chatbot into the Physical AI & Humanoid Robotics textbook. All file paths are relative to the Docusaurus root directory as specified.* 

## Prompt

# RAG Chatbot Technical Specification ## Physical AI & Humanoid Robotics Textbook **Version:** 1.0 **Date:** December 5, 2025 **Project Root:** `C:\new\physical-ai-robotics-textbook\docusaurus` ---
## 1. PROJECT STRUCTURE ### 1.1 Directory Organization All RAG chatbot components MUST be integrated within the existing Docusaurus structure: ``` C:\new\physical-ai-robotics-textbook\docusaurus\ 
â”œâ”€â”€ src\ 
â”‚ â”œâ”€â”€ components\ 
â”‚ â”‚ â”œâ”€â”€ ChatBot\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatBot.tsx # Main chatbot component 
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatBot.module.css # Chatbot styles 
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatWidget.tsx # Minimizable widget wrapper 
â”‚ â”‚ â”‚ â”œâ”€â”€ MessageList.tsx # Chat message display 
â”‚ â”‚ â”‚ â”œâ”€â”€ MessageInput.tsx # User input component 
â”‚ â”‚ â”‚ â”œâ”€â”€ SelectionHandler.tsx # Text selection integration 
â”‚ â”‚ â”‚ â”œâ”€â”€ CitationLink.tsx # Source citation component 
â”‚ â”‚ â”‚ â””â”€â”€ types.ts # TypeScript interfaces 
â”‚ â”‚ â””â”€â”€ [other existing components...] 
â”‚ â”œâ”€â”€ pages\ 
â”‚ â”‚ â””â”€â”€ api\ 
â”‚ â”‚ â””â”€â”€ chat.ts # API route (if using Docusaurus API) 
â”‚ â””â”€â”€ theme\ 
â”‚ â””â”€â”€ Root.tsx # Global chatbot injection point 
â”œâ”€â”€ static\ 
â”‚ â””â”€â”€ chatbot\ 
â”‚ â”œâ”€â”€ icons\ 
â”‚ â”‚ â”œâ”€â”€ bot-icon.svg 
â”‚ â”‚ â”œâ”€â”€ send-icon.svg 
â”‚ â”‚ â””â”€â”€ minimize-icon.svg 
â”‚ â””â”€â”€ audio\ 
â”‚ â””â”€â”€ notification.mp3 
â”œâ”€â”€ backend\ 
â”‚ â”œâ”€â”€ main.py # FastAPI application entry 
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies 
â”‚ â”œâ”€â”€ .env.example # Environment variables template 
â”‚ â”œâ”€â”€ app\ 
â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”œâ”€â”€ config.py # Configuration management 
â”‚ â”‚ â”œâ”€â”€ models\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”‚ â”œâ”€â”€ database.py # Neon Postgres connection 
â”‚ â”‚ â”‚ â””â”€â”€ schemas.py # Pydantic models 
â”‚ â”‚ â”œâ”€â”€ services\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”‚ â”œâ”€â”€ qdrant_service.py # Vector DB operations 
â”‚ â”‚ â”‚ â”œâ”€â”€ openai_service.py # OpenAI API integration 
â”‚ â”‚ â”‚ â”œâ”€â”€ rag_pipeline.py # RAG orchestration 
â”‚ â”‚ â”‚ â””â”€â”€ analytics_service.py # Usage tracking 
â”‚ â”‚ â”œâ”€â”€ routes\ 
â”‚ â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”‚ â”œâ”€â”€ chat.py # Chat endpoints 
â”‚ â”‚ â”‚ â””â”€â”€ health.py # Health check 
â”‚ â”‚ â””â”€â”€ utils\ 
â”‚ â”‚ â”œâ”€â”€ __init__.py 
â”‚ â”‚ â”œâ”€â”€ embeddings.py # Embedding utilities 
â”‚ â”‚ â”œâ”€â”€ chunking.py # Text chunking logic 
â”‚ â”‚ â””â”€â”€ logger.py # Logging configuration 
â”‚ â””â”€â”€ scripts\ 
â”‚ â”œâ”€â”€ index_textbook.py # Populate Qdrant with content 
â”‚ â”œâ”€â”€ test_pipeline.py # RAG pipeline testing 
â”‚ â””â”€â”€ migrate_db.py # Database migrations 
â”œâ”€â”€ docs\ 
â”‚ â””â”€â”€ [existing textbook content...] 
â”œâ”€â”€ docusaurus.config.js # Add chatbot client module 
â”œâ”€â”€ package.json # Add frontend dependencies 
â””â”€â”€ README_CHATBOT.md # Chatbot setup guide ``` ### 1.2 Key Integration Points #### A. Frontend Integration (Docusaurus) **Location:** `src/theme/Root.tsx` ```tsx import React from 'react'; import ChatWidget from '@site/src/components/ChatBot/ChatWidget'; export default function Root({children}) { return ( <> {children} <ChatWidget /> </> ); } ``` #### B. Backend Deployment **Separate Repository:** Backend will be deployed independently but configured to accept requests from Docusaurus domain **CORS Configuration:** ```python # backend/main.py from fastapi.middleware.cors import CORSMiddleware app.add_middleware( CORSMiddleware, allow_origins=[ "https://your-textbook-domain.com", "http://localhost:3000" # Development ], allow_credentials=True, allow_methods=["*"], allow_headers=["*"], ) ``` --- ## 2. COMPONENT SPECIFICATIONS ### 2.1 Frontend Components #### A. ChatBot.tsx **Path:** `src/components/ChatBot/ChatBot.tsx` **Responsibilities:** - Main chat interface container - Message state management - API communication with backend - Error handling and loading states **Key Props:** ```typescript interface ChatBotProps { initialOpen?: boolean; selectedText?: string; selectionContext?: { chapter: string; section: string; }; } ``` **State Management:** ```typescript interface ChatState { messages: Message[]; isLoading: boolean; error: string | null; mode: 'general' | 'selected_text'; sessionId: string; } ``` #### B. SelectionHandler.tsx **Path:** `src/components/ChatBot/SelectionHandler.tsx` **Functionality:** - Listen for text selection events on documentation pages - Context menu integration (right-click) - Keyboard shortcut handler (Ctrl+Shift+A) - Extract surrounding context (chapter/section) **Implementation Pattern:** ```typescript useEffect(() => { const handleSelection = () => { const selection = window.getSelection(); if (selection && selection.toString().length > 10) { // Show "Ask about this" option setSelectedText(selection.toString()); setSelectionContext(extractContext()); } }; document.addEventListener('mouseup', handleSelection); return () => document.removeEventListener('mouseup', handleSelection); }, []); ``` #### C. CitationLink.tsx **Path:** `src/components/ChatBot/CitationLink.tsx` **Purpose:** Render clickable source citations **Props:** ```typescript interface CitationLinkProps { chapter: string; section: string; onClick: () => void; } ``` **Output Format:** ``` [Chapter 2, Section 2.3] â†’ Clicking navigates to /docs/chapter-2#section-2-3 ``` ### 2.2 Backend Services #### A. RAG Pipeline Service **Path:** `backend/app/services/rag_pipeline.py` **Core Functions:** ```python class RAGPipeline: def __init__(self): self.qdrant = QdrantService() self.openai = OpenAIService() self.db = DatabaseService() async def process_general_query( self, query: str, session_id: str, user_background: dict = None ) -> ChatResponse: """ 1. Expand query with technical synonyms 2. Retrieve top-k chunks from Qdrant (k=5) 3. Rerank by relevance 4. Generate response with citations 5. Log to Neon Postgres """ pass async def process_selected_text_query( self, query: str, selected_text: str, context: dict, session_id: str ) -> ChatResponse: """ 1. Create ephemeral embedding from selection 2. Restrict retrieval to selection's chapter/section 3. Generate focused response 4. Ensure no context bleeding """ pass ``` #### B. Qdrant Service **Path:** `backend/app/services/qdrant_service.py` **Configuration:** ```python from qdrant_client import QdrantClient from qdrant_client.models import Distance, VectorParams class QdrantService: def __init__(self): self.client = QdrantClient( url=os.getenv("QDRANT_URL"), api_key=os.getenv("QDRANT_API_KEY") ) self.collection_name = "physical_ai_textbook_v1" def create_collection(self): self.client.create_collection( collection_name=self.collection_name, vectors_config=VectorParams( size=3072, # text-embedding-3-large dimension distance=Distance.COSINE ) ) async def search( self, query_vector: List[float], limit: int = 5, filters: dict = None ) -> List[ScoredPoint]: """Hybrid search with metadata filtering""" pass ``` **Metadata Structure:** ```python { "chapter": "Chapter 2", "section": "2.3 Isaac Sim Setup", "subsection": "Hardware Requirements", "type": "hardware_spec", # concept | code | hardware_spec | example "keywords": ["RTX", "GPU", "Isaac Sim", "NVIDIA"], "page_number": 45, "textbook_version": "1.0" } ``` #### C. OpenAI Service **Path:** `backend/app/services/openai_service.py` **Functions:** ```python from openai import AsyncOpenAI class OpenAIService: def __init__(self): self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY")) async def create_embedding(self, text: str) -> List[float]: """Generate embedding using text-embedding-3-large""" response = await self.client.embeddings.create( model="text-embedding-3-large", input=text ) return response.data[0].embedding async def generate_response( self, query: str, context_chunks: List[str], mode: str = "general" ) -> str: """Generate response using GPT-4 with retrieved context""" system_prompt = self._build_system_prompt(mode) user_prompt = self._build_user_prompt(query, context_chunks) response = await self.client.chat.completions.create( model="gpt-4-turbo-preview", messages=[ {"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt} ], temperature=0.3, max_tokens=800 ) return response.choices[0].message.content ``` **System Prompt Template:** ```python SYSTEM_PROMPT = """You are an expert teaching assistant for the Physical AI & Humanoid Robotics course. RULES: 1. Answer ONLY using the provided textbook context 2. ALWAYS cite sources in format: [Chapter X, Section Y.Z] 3. If context insufficient, explicitly state: "This detail is not covered in the provided textbook section." 4. Use technical precision (correct ROS 2 terminology, version numbers) 5. Explain complex concepts step-by-step 6. Include relevant code snippets when available in context CONTEXT FROM TEXTBOOK: {context_chunks} STUDENT QUESTION: {query} Provide a clear, educational response with proper citations.""" ``` --- ## 3. DATABASE SCHEMA (NEON POSTGRES) ### 3.1 Tables Definition **File:** `backend/app/models/database.py` ```sql -- User Sessions CREATE TABLE user_sessions ( session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(), user_id UUID NULL, -- NULL for anonymous users created_at TIMESTAMP DEFAULT NOW(), last_activity TIMESTAMP DEFAULT NOW(), user_background JSONB, -- {hardware_experience, programming_level} device_info JSONB -- User agent, screen size for analytics ); -- Query Logs CREATE TABLE query_logs ( query_id UUID PRIMARY KEY DEFAULT gen_random_uuid(), session_id UUID REFERENCES user_sessions(session_id) ON DELETE CASCADE, query_text TEXT NOT NULL, query_type VARCHAR(50) CHECK (query_type IN ('general', 'selected_text')), selected_context TEXT, -- Stored if selected_text mode selected_metadata JSONB, -- {chapter, section} retrieved_chunks JSONB, -- Array of chunk IDs used response_text TEXT, citations JSONB, -- [{chapter, section, url}] response_time_ms INTEGER, user_feedback INTEGER CHECK (user_feedback IN (-1, 0, 1)), created_at TIMESTAMP DEFAULT NOW(), INDEX idx_session (session_id), INDEX idx_created (created_at), INDEX idx_feedback (user_feedback) ); -- Popular Topics (Materialized View) CREATE MATERIALIZED VIEW popular_topics AS SELECT selected_metadata->>'chapter' AS chapter, selected_metadata->>'section' AS section, COUNT(*) AS query_count, AVG(CASE WHEN user_feedback = 1 THEN 1.0 ELSE 0.0 END) AS satisfaction_rate, MAX(created_at) AS last_queried FROM query_logs WHERE created_at > NOW() - INTERVAL '30 days' GROUP BY chapter, section ORDER BY query_count DESC; -- Refresh schedule: Every 6 hours ``` ### 3.2 SQLAlchemy Models **File:** `backend/app/models/schemas.py` ```python from sqlalchemy import Column, String, Integer, Float, DateTime, Text, ForeignKey from sqlalchemy.dialects.postgresql import UUID, JSONB from sqlalchemy.ext.declarative import declarative_base import uuid Base = declarative_base() class UserSession(Base): __tablename__ = "user_sessions" session_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4) user_id = Column(UUID(as_uuid=True), nullable=True) created_at = Column(DateTime, server_default=func.now()) last_activity = Column(DateTime, server_default=func.now(), onupdate=func.now()) user_background = Column(JSONB) device_info = Column(JSONB) class QueryLog(Base): __tablename__ = "query_logs" query_id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4) session_id = Column(UUID(as_uuid=True), ForeignKey("user_sessions.session_id")) query_text = Column(Text, nullable=False) query_type = Column(String(50), nullable=False) selected_context = Column(Text) selected_metadata = Column(JSONB) retrieved_chunks = Column(JSONB) response_text = Column(Text) citations = Column(JSONB) response_time_ms = Column(Integer) user_feedback = Column(Integer) created_at = Column(DateTime, server_default=func.now()) ``` --- ## 4. API ENDPOINTS ### 4.1 Chat Endpoint **File:** `backend/app/routes/chat.py` ```python from fastapi import APIRouter, HTTPException, Depends from pydantic import BaseModel router = APIRouter(prefix="/api/v1", tags=["chat"]) class GeneralQueryRequest(BaseModel): query: str session_id: str user_background: dict = None class SelectedTextQueryRequest(BaseModel): query: str selected_text: str selection_context: dict # {chapter, section} session_id: str class ChatResponse(BaseModel): answer: str citations: List[dict] # [{chapter, section, url}] confidence: float mode: str response_time_ms: int @router.post("/chat/general", response_model=ChatResponse) async def general_query(request: GeneralQueryRequest): """Handle general textbook queries""" try: pipeline = RAGPipeline() response = await pipeline.process_general_query( query=request.query, session_id=request.session_id, user_background=request.user_background ) return response except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @router.post("/chat/selected", response_model=ChatResponse) async def selected_text_query(request: SelectedTextQueryRequest): """Handle queries about selected text""" if len(request.selected_text) > 2000: raise HTTPException( status_code=400, detail="Selected text exceeds 2000 character limit" ) pipeline = RAGPipeline() response = await pipeline.process_selected_text_query( query=request.query, selected_text=request.selected_text, context=request.selection_context, session_id=request.session_id ) return response @router.post("/chat/feedback") async def submit_feedback(query_id: str, feedback: int): """Record user feedback (thumbs up/down)""" # Update query_logs table pass ### 4.2 Health Check **File:** `backend/app/routes/health.py` ```python @router.get("/health") async def health_check(): """Check service health""" checks = { "qdrant": await check_qdrant_connection(), "postgres": await check_postgres_connection(), "openai": await check_openai_api() } if all(checks.values()): return {"status": "healthy", "checks": checks} else: raise HTTPException(status_code=503, detail=checks) ``` --- ## 5. INDEXING PIPELINE ### 5.1 Textbook Content Extraction **File:** `backend/scripts/index_textbook.py` **Process:** ```python import os from pathlib import Path from app.services.qdrant_service import QdrantService from app.services.openai_service import OpenAIService from app.utils.chunking import chunk_markdown async def index_textbook_content(): """ Extract and index all Markdown files from Docusaurus docs folder """ docs_path = Path("C:/new/physical-ai-robotics-textbook/docusaurus/docs") qdrant = QdrantService() openai = OpenAIService() # Ensure collection exists qdrant.create_collection() for md_file in docs_path.rglob("*.md"): print(f"Processing: {md_file}") # Extract metadata from frontmatter metadata = extract_frontmatter(md_file) # Read content with open(md_file, 'r', encoding='utf-8') as f: content = f.read() # Chunk content (800-1200 tokens, 200 overlap) chunks = chunk_markdown(content, metadata) # Create embeddings and upload to Qdrant for chunk in chunks: embedding = await openai.create_embedding(chunk.text) await qdrant.upsert_point( vector=embedding, payload={ "text": chunk.text, "chapter": chunk.chapter, "section": chunk.section, "subsection": chunk.subsection, "type": chunk.type, "keywords": chunk.keywords, "file_path": str(md_file.relative_to(docs_path)), "url": generate_doc_url(md_file) } ) print(f"Indexed {len(chunks)} chunks from {md_file.name}") def extract_frontmatter(md_file: Path) -> dict: """Parse YAML frontmatter from Markdown file""" import yaml with open(md_file, 'r', encoding='utf-8') as f: content = f.read() if content.startswith('---\n'): _, frontmatter, _ = content.split('---\n', 2) return yaml.safe_load(frontmatter) return {} def generate_doc_url(md_file: Path) -> str: """Convert file path to Docusaurus URL""" # Example: docs/chapter-2/section-2-3.md â†’ /docs/chapter-2/section-2-3 relative_path = md_file.relative_to(docs_path) url_path = str(relative_path).replace('\\', '/').replace('.md', '') return f"/docs/{url_path}" ``` ### 5.2 Chunking Strategy **File:** `backend/app/utils/chunking.py` ```python from typing import List import tiktoken class TextChunk: def __init__(self, text: str, metadata: dict): self.text = text self.chapter = metadata.get('chapter', 'Unknown') self.section = metadata.get('section', 'Unknown') self.subsection = metadata.get('subsection', '') self.type = self._classify_type(text) self.keywords = self._extract_keywords(text) def _classify_type(self, text: str) -> str: """Classify chunk type based on content patterns""" if '```' in text: return 'code' elif any(hw in text.lower() for hw in ['jetson', 'gpu', 'ram', 'ssd']): return 'hardware_spec' elif 'example:' in text.lower() or 'for instance' in text.lower(): return 'example' else: return 'concept' def _extract_keywords(self, text: str) -> List[str]: """Extract technical keywords for filtering""" # Simple keyword extraction (can be enhanced with NLP) keywords = [] technical_terms = ['ROS 2', 'Isaac Sim', 'Gazebo', 'VSLAM', 'Jetson', 'URDF'] for term in technical_terms: if term.lower() in text.lower(): keywords.append(term) return keywords def chunk_markdown(content: str, metadata: dict) -> List[TextChunk]: """ Chunk Markdown content with respect to structure Strategy: - Target: 800-1200 tokens per chunk - Overlap: 200 tokens - Respect boundaries: Headers, code blocks, lists """ encoding = tiktoken.get_encoding("cl100k_base") # Split by headers first sections = split_by_headers(content) chunks = [] for section in sections: section_metadata = {**metadata, 'section': section['title']} tokens = encoding.encode(section['content']) if len(tokens) <= 1200: # Section fits in one chunk chunks.append(TextChunk(section['content'], section_metadata)) else: # Split section into overlapping chunks chunk_size = 1000 overlap = 200 for i in range(0, len(tokens), chunk_size - overlap): chunk_tokens = tokens[i:i + chunk_size] chunk_text = encoding.decode(chunk_tokens) chunks.append(TextChunk(chunk_text, section_metadata)) return chunks def split_by_headers(content: str) -> List[dict]: """Split Markdown by headers (##, ###)""" import re header_pattern = r'^(#{2,3})\s+(.+)' lines = content.split('\n') sections = [] current_section = {'title': 'Introduction', 'content': ''} for line in lines: match = re.match(header_pattern, line, re.MULTILINE) if match: # Save previous section if current_section['content'].strip(): sections.append(current_section) # Start new section current_section = { 'title': match.group(2), 'content': '' } else: current_section['content'] += line + '\n' # Add last section if current_section['content'].strip(): sections.append(current_section) return sections ``` --- ## 6. DEPLOYMENT CONFIGURATION ### 6.1 Environment Variables **File:** `backend/.env.example` ```bash # OpenAI OPENAI_API_KEY=sk-proj-... # Qdrant Cloud QDRANT_URL=https://your-cluster.qdrant.io QDRANT_API_KEY=your-api-key QDRANT_COLLECTION=physical_ai_textbook_v1 # Neon Postgres DATABASE_URL=postgresql://user:password@ep-xxx.us-east-2.aws.neon.tech/chatbot_db?sslmode=require # CORS ALLOWED_ORIGINS=https://your-textbook-domain.com,http://localhost:3000 # Rate Limiting RATE_LIMIT_REQUESTS=50 # per minute per session RATE_LIMIT_WINDOW=60 # seconds # Monitoring SENTRY_DSN=https://xxx@xxx.ingest.sentry.io/xxx # Optional LOG_LEVEL=INFO ``` ### 6.2 Backend Deployment (Railway) **File:** `backend/railway.json` ```json { "$schema": "https://railway.app/railway.schema.json", "build": { "builder": "NIXPACKS", "buildCommand": "pip install -r requirements.txt" }, "deploy": { "startCommand": "uvicorn main:app --host 0.0.0.0 --port $PORT", "healthcheckPath": "/health", "healthcheckTimeout": 30, "restartPolicyType": "ON_FAILURE", "restartPolicyMaxRetries": 3 } } ``` **File:** `backend/requirements.txt` ```txt fastapi==0.109.0 uvicorn[standard]==0.27.0 python-multipart==0.0.6 pydantic==2.5.3 pydantic-settings==2.1.0 # OpenAI openai==1.10.0 # Vector DB qdrant-client==1.7.3 # Database sqlalchemy==2.0.25 asyncpg==0.29.0 psycopg2-binary==2.9.9 alembic==1.13.1 # Utilities python-dotenv==1.0.0 tiktoken==0.5.2 pyyaml==6.0.1 httpx==0.26.0 # Monitoring sentry-sdk[fastapi]==1.40.0 ``` ### 6.3 Frontend Configuration **File:** `docusaurus/package.json` (additions) ```json { "dependencies": { "@docusaurus/core": "3.0.0", "@docusaurus/preset-classic": "3.0.0", // ... existing dependencies // Chatbot dependencies "axios": "^1.6.5", "uuid": "^9.0.1", "react-markdown": "^9.0.1", "remark-gfm": "^4.0.0", "highlight.js": "^11.9.0" } } ``` **File:** `docusaurus/docusaurus.config.js` (additions) ```javascript module.exports = { // ... existing config customFields: { chatbotApiUrl: process.env.CHATBOT_API_URL || 'http://localhost:8000', }, scripts: [ // Optional: Analytics for chatbot usage { src: 'https://plausible.io/js/script.js', defer: true, 'data-domain': 'your-textbook-domain.com' } ], }; ``` --- ## 7. TESTING REQUIREMENTS ### 7.1 Unit Tests **File:** `backend/tests/test_rag_pipeline.py` ```python import pytest from app.services.rag_pipeline import RAGPipeline @pytest.mark.asyncio async def test_general_query_with_citations(): """Test that general queries return proper citations""" pipeline = RAGPipeline() response = await pipeline.process_general_query( query="What is ROS 2?", session_id="test-session-123" ) assert response.answer is not None assert len(response.citations) > 0 assert all('chapter' in c for c in response.citations) assert response.confidence > 0.5 @pytest.mark.asyncio async def test_selected_text_no_context_bleeding(): """Ensure selected text mode doesn't retrieve from other chapters""" pipeline = RAGPipeline() selected_text = "NVIDIA Jetson Orin Nano specifications..." response = await pipeline.process_selected_text_query( query="Why is 64GB RAM recommended?", selected_text=selected_text, context={"chapter": "Chapter 4", "section": "Hardware"}, session_id="test-session-456" ) # All citations should be from Chapter 4 only assert all(c['chapter'] == "Chapter 4" for c in response.citations) ``` **File:** `backend/tests/test_chunking.py` ```python from app.utils.chunking import chunk_markdown def test_chunking_respects_token_limit(): """Test that chunks stay within 800-1200 token range""" content = "# Chapter 1\n\n" + "Lorem ipsum " * 2000 metadata = {"chapter": "Chapter 1"} chunks = chunk_markdown(content, metadata) for chunk in chunks: token_count = len(encoding.encode(chunk.text)) assert 500 <= token_count <= 1300 # Allow some buffer def test_chunking_preserves_code_blocks(): """Ensure code blocks aren't split mid-block""" content = """ ## Setup ```python def example(): return \"test\" ``` More text here. """ chunks = chunk_markdown(content, {"chapter": "Chapter 1"}) # Code block should be in one chunk code_chunks = [c for c in chunks if '```python' in c.text] assert any('def example():' in c.text and 'return \"test\"' in c.text for c in code_chunks) ``` ### 7.2 Integration Tests **File:** `backend/tests/test_api_endpoints.py` ```python from fastapi.testclient import TestClient from main import app client = TestClient(app) def test_general_query_endpoint(): """Test /api/v1/chat/general endpoint""" response = client.post( "/api/v1/chat/general", json={ "query": "What is Gazebo?", "session_id": "test-123" } ) assert response.status_code == 200 data = response.json() assert "answer" in data assert "citations" in data assert len(data["citations"]) > 0 def test_selected_text_query_endpoint(): """Test /api/v1/chat/selected endpoint""" response = client.post( "/api/v1/chat/selected", json={ "query": "Why this hardware?", "selected_text": "Jetson Orin Nano (8GB)...", "selection_context": { "chapter": "Chapter 4", "section": "Hardware Requirements" }, "session_id": "test-456" } ) assert response.status_code == 200 data = response.json() assert data["mode"] == "selected_text" def test_rate_limiting(): """Test that rate limiting works""" session_id = "rate-limit-test" # Send 60 requests (over limit of 50/min) responses = [] for i in range(60): response = client.post( "/api/v1/chat/general", json={"query": f"Test {i}", "session_id": session_id} ) responses.append(response.status_code) # Last 10 should be rate limited (429) assert 429 in responses[-10:] ``` ### 7.3 User Acceptance Testing **Test Cases Document:** `docs/CHATBOT_TEST_CASES.md` | Test ID | Scenario | Expected Result | Pass/Fail | |---------|----------|-----------------|-----------| | UAT-01 | Ask "What is ROS 2?" | Returns definition with [Chapter X] citation | | | UAT-02 | Select hardware specs paragraph, ask "Why Jetson?" | Answers from selection only | | | UAT-03 | Ask about topic not in textbook | Responds "This is not covered..." | | | UAT-04 | Send empty query | Shows validation error | | | UAT-05 | Select > 2000 chars, ask question | Returns 400 error with message | | | UAT-06 | Click citation link | Navigates to correct doc section | | | UAT-07 | Submit thumbs up/down feedback | Updates database record | | | UAT-08 | Open chatbot on mobile | Responsive full-screen layout | | | UAT-09 | Use keyboard shortcut (Ctrl+Shift+A) | Opens chatbot with selection | | | UAT-10 | Ask code-related question | Includes syntax-highlighted code snippet | | --- ## 8. MONITORING & ANALYTICS ### 8.1 Logging Configuration **File:** `backend/app/utils/logger.py` ```python import logging import sys from pythonjsonlogger import jsonlogger def setup_logger(): """Configure structured JSON logging""" logger = logging.getLogger("chatbot") logger.setLevel(logging.INFO) handler = logging.StreamHandler(sys.stdout) formatter = jsonlogger.JsonFormatter( fmt='%(asctime)s %(name)s %(levelname)s %(message)s', rename_fields={ "asctime": "timestamp", "levelname": "level" } ) handler.setFormatter(formatter) logger.addHandler(handler) return logger # Usage in services logger = setup_logger() logger.info("Query processed", extra={ "query_id": query_id, "response_time_ms": elapsed_time, "citations_count": len(citations) }) ``` ### 8.2 Analytics Dashboard Queries **File:** `backend/scripts/analytics_queries.sql` ```sql -- Most popular topics (last 30 days) SELECT selected_metadata->>'chapter' AS chapter, selected_metadata->>'section' AS section, COUNT(*) AS query_count, ROUND(AVG(response_time_ms)) AS avg_response_ms, ROUND(AVG(CASE WHEN user_feedback = 1 THEN 1.0 ELSE 0.0 END) * 100, 2) AS satisfaction_pct FROM query_logs WHERE created_at > NOW() - INTERVAL '30 days' GROUP BY chapter, section ORDER BY query_count DESC LIMIT 20; -- Daily active sessions SELECT DATE(created_at) AS date, COUNT(DISTINCT session_id) AS active_sessions, COUNT(*) AS total_queries, ROUND(AVG(response_time_ms)) AS avg_response_ms FROM query_logs WHERE created_at > NOW() - INTERVAL '90 days' GROUP BY DATE(created_at) ORDER BY date DESC; -- Query type distribution SELECT query_type, COUNT(*) AS count, ROUND(AVG(response_time_ms)) AS avg_response_ms, ROUND(AVG(CASE WHEN user_feedback = 1 THEN 1.0 ELSE 0.0 END) * 100, 2) AS satisfaction_pct FROM query_logs WHERE created_at > NOW() - INTERVAL '7 days' GROUP BY query_type; -- User feedback summary SELECT CASE WHEN user_feedback = 1 THEN 'Thumbs Up' WHEN user_feedback = -1 THEN 'Thumbs Down' ELSE 'No Feedback' END AS feedback_type, COUNT(*) AS count, ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage FROM query_logs WHERE created_at > NOW() - INTERVAL '30 days' GROUP BY user_feedback; ``` --- ## 9. MAINTENANCE PROCEDURES ### 9.1 Re-indexing Workflow **When to Re-index:** - Textbook content updated (new chapters, corrections) - Embedding model upgraded (e.g., text-embedding-3-large â†’ text-embedding-4) - Chunking strategy changed **Process:** ```bash # 1. Backup existing Qdrant collection python backend/scripts/backup_qdrant.py # 2. Create new collection with version suffix # Modify QDRANT_COLLECTION in .env: physical_ai_textbook_v2 # 3. Run indexing script python backend/scripts/index_textbook.py # 4. Validate new collection python backend/scripts/test_pipeline.py --collection physical_ai_textbook_v2 # 5. Update production environment variable # Switch QDRANT_COLLECTION to v2 # 6. Monitor for 24 hours, delete old collection if stable ``` ### 9.2 Cost Optimization **Monthly Cost Estimate (Free Tier):** - **Qdrant Cloud**: Free (1GB storage, 1M vectors) - **Neon Postgres**: Free (0.5GB storage, 100 hours compute) - **OpenAI API**: ~$20-50 (assuming 10k queries/month) - Embeddings: ~$0.50 (text-embedding-3-large) - Completions: ~$20-50 (GPT-4-turbo at $0.01/1K tokens) **Optimization Strategies:** 1. **Caching**: Store embeddings of common queries 2. **Response Caching**: Cache responses for identical queries (30 min TTL) 3. **Batch Embeddings**: When re-indexing, batch requests (50 chunks per API call) 4. **Model Selection**: Use GPT-3.5-turbo for simple queries (detected by keyword matching) ```python # Example caching implementation from functools import lru_cache import hashlib @lru_cache(maxsize=1000) def get_cached_response(query_hash: str): """Cache responses for 30 minutes""" # Check Redis/in-memory cache pass async def process_query(query: str): query_hash = hashlib.md5(query.encode()).hexdigest() cached = get_cached_response(query_hash) if cached: return cached # Generate new response response = await pipeline.process_general_query(query) cache_response(query_hash, response, ttl=1800) # 30 min return response ``` --- ## 10. SUCCESS METRICS ### 10.1 Performance KPIs | Metric | Target | Measurement | |--------|--------|-------------| | **Response Time (P95)** | < 3 seconds | Monitor `response_time_ms` column | | **Accuracy** | > 90% | Manual review of 100 random samples/week | | **Citation Rate** | 100% | Automated check: all responses must include `[Chapter X]` | | **User Satisfaction** | > 70% thumbs up | `AVG(user_feedback)` from query_logs | | **Error Rate** | < 2% | 5xx errors / total requests | | **Uptime** | > 99% | Railway/Vercel monitoring | ### 10.2 Usage Metrics | Metric | Target | Measurement | |--------|--------|-------------| | **Daily Active Users** | 50+ | `COUNT(DISTINCT session_id)` per day | | **Queries per Session** | 3+ | `AVG(queries/session)` | | **Engagement Rate** | > 30% of page visitors | Google Analytics + chatbot opens | | **Selected Text Usage** | > 20% of queries | `query_type='selected_text' / total` | ### 10.3 Quality Assurance **Weekly Review Process:** 1. Sample 50 random queries with low feedback scores 2. Evaluate response quality (accuracy, citation correctness) 3. Identify common failure patterns 4. Update prompt templates or re-index if needed 5. Document improvements in changelog **Quarterly Review:** - Analyze popular topics â†’ Suggest textbook enhancements - Review cost trends â†’ Optimize if exceeding $100/month - User surveys â†’ Collect qualitative feedback - A/B test prompt variations --- ## 11. DEPLOYMENT CHECKLIST ### 11.1 Pre-Launch (Complete Before Production) **Backend Setup:** - [ ] Qdrant collection created and populated with all textbook chapters - [ ] Neon Postgres database schema deployed - [ ] OpenAI API key configured in Railway environment - [ ] CORS configured for production domain - [ ] Rate limiting middleware enabled (50 req/min per session) - [ ] Health check endpoint returning 200 - [ ] Sentry error tracking configured - [ ] SSL certificate valid **Frontend Setup:** - [ ] ChatBot component integrated in `src/theme/Root.tsx` - [ ] Text selection handler tested on all documentation pages - [ ] Citation links navigate correctly to docs - [ ] Mobile responsive design verified (< 768px width) - [ ] Loading states implemented (skeleton screens) - [ ] Error messages user-friendly - [ ] Keyboard shortcuts functional (Ctrl+Shift+A) **Testing:** - [ ] All unit tests passing (`pytest backend/tests/`) - [ ] Integration tests passing (API endpoints) - [ ] 20 user acceptance test cases completed - [ ] Load testing: 100 concurrent requests handled - [ ] Browser compatibility: Chrome, Firefox, Safari, Edge **Documentation:** - [ ] README_CHATBOT.md created with setup instructions - [ ] API documentation generated (FastAPI auto-docs) - [ ] Analytics dashboard queries documented - [ ] Runbook for common issues created ### 11.2 Launch Day **Deployment Steps:** 1. Deploy backend to Railway 2. Update `CHATBOT_API_URL` in Docusaurus environment 3. Deploy Docusaurus site to production 4. Monitor logs for first 100 queries 5. Test from production domain 6. Announce to users (in-app banner) **Monitoring:** - [ ] Check Railway logs every hour (first 24h) - [ ] Monitor Sentry for errors - [ ] Track response times in Neon Postgres - [ ] Review first 50 user feedback scores ### 11.3 Post-Launch (Week 1) - [ ] Collect user feedback survey responses - [ ] Analyze query logs for common patterns - [ ] Identify and fix any edge cases - [ ] Update FAQ based on common queries - [ ] Prepare iteration plan for improvements --- ## 12. APPENDIX ### 12.1 Example Frontend Component **File:** `src/components/ChatBot/ChatBot.tsx` ```typescript import React, { useState, useEffect, useRef } from 'react'; import axios from 'axios'; import { v4 as uuidv4 } from 'uuid'; import styles from './ChatBot.module.css'; import MessageList from './MessageList'; import MessageInput from './MessageInput'; interface Message { id: string; role: 'user' | 'assistant'; content: string; citations?: Citation[]; timestamp: Date; } interface Citation { chapter: string; section: string; url: string; } interface ChatBotProps { selectedText?: string; selectionContext?: { chapter: string; section: string; }; } const ChatBot: React.FC<ChatBotProps> = ({ selectedText, selectionContext }) => { const [messages, setMessages] = useState<Message[]>([]); const [isLoading, setIsLoading] = useState(false); const [sessionId] = useState(() => uuidv4()); const messagesEndRef = useRef<HTMLDivElement>(null); const apiUrl = process.env.CHATBOT_API_URL || 'http://localhost:8000'; useEffect(() => { // Show welcome message if (messages.length === 0) { setMessages([{ id: uuidv4(), role: 'assistant', content: selectedText ? `I'll help you understand this section. What would you like to know?` : `Hi! I'm your Physical AI textbook assistant. Ask me anything about ROS 2, Isaac Sim, or humanoid robotics!`, timestamp: new Date() }]); } }, [selectedText]); useEffect(() => { messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' }); }, [messages]); const handleSendMessage = async (content: string) => { // Add user message const userMessage: Message = { id: uuidv4(), role: 'user', content, timestamp: new Date() }; setMessages(prev => [...prev, userMessage]); setIsLoading(true); try { const endpoint = selectedText ? `${apiUrl}/api/v1/chat/selected` : `${apiUrl}/api/v1/chat/general`; const payload = selectedText ? { query: content, selected_text: selectedText, selection_context: selectionContext, session_id: sessionId } : { query: content, session_id: sessionId }; const response = await axios.post(endpoint, payload); // Add assistant message const assistantMessage: Message = { id: uuidv4(), role: 'assistant', content: response.data.answer, citations: response.data.citations, timestamp: new Date() }; setMessages(prev => [...prev, assistantMessage]); } catch (error) { console.error('Chat error:', error); // Show error message setMessages(prev => [...prev, { id: uuidv4(), role: 'assistant', content: 'Sorry, I encountered an error. Please try again or rephrase your question.', timestamp: new Date() }]); } finally { setIsLoading(false); } }; const handleFeedback = async (messageId: string, feedback: number) => { try { await axios.post(`${apiUrl}/api/v1/chat/feedback`, { query_id: messageId, feedback }); } catch (error) { console.error('Feedback error:', error); } }; return ( <div className={styles.chatContainer}> <div className={styles.header}> <h3>ðŸ“š Textbook Assistant</h3> {selectedText && ( <span className={styles.modeIndicator}> Selection Mode </span> )} </div> <MessageList messages={messages} onFeedback={handleFeedback} /> <div ref={messagesEndRef} /> <MessageInput onSend={handleSendMessage} disabled={isLoading} placeholder={selectedText ? "Ask about this section..." : "Ask a question about the textbook..." } /> </div> ); }; export default ChatBot; ``` ### 12.2 Quick Start Commands ```bash # Backend Setup cd C:\new\physical-ai-robotics-textbook\docusaurus\backend python -m venv venv venv\Scripts\activate # Windows pip install -r requirements.txt cp .env.example .env # Edit .env with your API keys # Index textbook content python scripts/index_textbook.py # Run backend locally uvicorn main:app --reload --port 8000 # Frontend Setup (in separate terminal) cd C:\new\physical-ai-robotics-textbook\docusaurus npm install npm start # Starts on http://localhost:3000 # Run tests cd backend pytest tests/ -v # Deploy backend to Railway railway login railway link railway up # Deploy frontend to Vercel/GitHub Pages npm run build # Follow deployment platform instructions ``` --- ## DOCUMENT CONTROL **Version:** 1.0 **Last Updated:** December 5, 2025 **Next Review:** January 5, 2026 **Owner:** Development Team **Approvers:** [Pending] --- *This specification provides a complete implementation guide for integrating the RAG chatbot into the Physical AI & Humanoid Robotics textbook. All file paths are relative to the Docusaurus root directory as specified.*